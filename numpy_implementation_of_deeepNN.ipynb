{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h23Um-A1xXWp"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EFwXYqG7yKsU"
   },
   "outputs": [],
   "source": [
    "def initialization(next_layer_dims, prev_layer_dims):\n",
    "    W  = np.random.randn(next_layer_dims, prev_layer_dims)\n",
    "    b = np.zeros([next_layer_dims,1])\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "unjEhFSMy40R"
   },
   "outputs": [],
   "source": [
    "def forward_prop(A_prev, W, b):\n",
    "    Z = np.dot(W, A_prev)+b\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OnD_Pn8vCXLF"
   },
   "outputs": [],
   "source": [
    "def activation_function(Z, activation):\n",
    "    if activation == 'sigmoid':\n",
    "        A = 1/(1+np.exp(-Z))\n",
    "\n",
    "    if activation == 'relu':\n",
    "        A = np.maximum(Z, 0)\n",
    "\n",
    "    return A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhIy7aPbETaA"
   },
   "outputs": [],
   "source": [
    "def cost_function(AL, label):\n",
    "\n",
    "    m = AL.shape[1]\n",
    "    cost = -1/m*(np.sum(label*np.log(AL)+(1-label)*np.log(1-AL)))\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GzRnIxfQeXkY"
   },
   "outputs": [],
   "source": [
    "def relu_grad(dA, A):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    print(A.shape)\n",
    "    print(dZ.shape[1])\n",
    "  \n",
    "    for i in range(dZ.shape[1]):\n",
    "        if A[i] == 0:\n",
    "            dZ[i] = 0\n",
    "     \n",
    "    return dZ\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5oKCcFE4C8uv"
   },
   "outputs": [],
   "source": [
    "def sigmoid_grad(A):\n",
    "    return A*(1-A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tDtlNTFUF4xR"
   },
   "outputs": [],
   "source": [
    "def backward_prop(dZ, A_prev, W ):\n",
    "\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1/m*(np.dot(dZ, A_prev.T))\n",
    "    db = 1/m*np.sum(dZ)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "    #dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BoqjBfDUKvNZ"
   },
   "outputs": [],
   "source": [
    "def training_model(no_of_layers, nL, X, Y, learning_rate, EPOCHS):\n",
    "\n",
    "    param= {}\n",
    "    results={\"A0\":X}\n",
    "    \n",
    "    Z = {}\n",
    "    #initializing parameters:\n",
    "    for l in range(1, no_of_layers):\n",
    "        param[\"W\" + str(l)], param[\"b\" + str(l)] = initialization(nL[l],nL[l-1])        \n",
    "\n",
    "    for epochs in range(EPOCHS):\n",
    "        for l in range(1, no_of_layers-1):\n",
    "            \n",
    "            #initialization of parameters \n",
    "            W = param[\"W\" + str(l)]\n",
    "            b = param[\"b\" + str(l)]\n",
    "\n",
    "            #forward propogation \n",
    "            Z[\"Z\"+ str(l)] = forward_prop(results[\"A\" + str(l-1)], W, b)\n",
    "            results[\"A\"+ str(l)] = activation_function( Z[\"Z\"+str(l)], activation=\"relu\")\n",
    "\n",
    "        Z[\"Z\" + str(no_of_layers-1)] = forward_prop(results[\"A\" + str(no_of_layers-2 )], param[\"W\"+str(no_of_layers-1)], param[\"b\"+str(no_of_layers-1)])\n",
    "        results[\"A\"+ str(no_of_layers-1)] = activation_function( Z[\"Z\"+ str(no_of_layers-1)], activation=\"sigmoid\")\n",
    "\n",
    "        #finding cost:\n",
    "\n",
    "        cost = cost_function(results[\"A\"+ str(no_of_layers-1)], Y)\n",
    "\n",
    "        print(cost)\n",
    "        #backward prop :\n",
    "        grads = {}\n",
    "\n",
    "        AL = results[\"A\"+ str(no_of_layers-1)]\n",
    "        print(AL.shape)\n",
    "        grads[\"dA\"+str(no_of_layers-1)] = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "        dZ = grads[\"dA\"+str(no_of_layers-1)]*sigmoid_grad(AL)\n",
    "        for l in range(no_of_layers-1,0,-1):\n",
    "            grads[\"dA\"+str(l)],  grads[\"dW\"+str(l)], grads[\"db\"+str(l)] = backward_prop(dZ, AL, param[\"W\"+str(l)] )\n",
    "            dZ = relu_grad(grads[\"dA\"+str(l)],results[\"A\"+str(l)])\n",
    "        # updating the parameters:\n",
    "\n",
    "        for l in range(1,no_of_layers):\n",
    "            param[\"W\"+str(l)] = param[\"W\"+str(l)] - learning_rate*grads[\"dW\"+str(l)]\n",
    "            param[\"b\"+str(l)] = param[\"b\"+str(l)] - learning_rate*grads[\"db\"+str(l)] \n",
    "            \n",
    "    return param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gfD8eQF4gn-D"
   },
   "outputs": [],
   "source": [
    "def predict(X, Y, param,no_of_layers):\n",
    "    for l in range(1, no_of_layers):\n",
    "            \n",
    "        #initialization of parameters \n",
    "        W = param[\"W\" + str(l)]\n",
    "        b = param[\"b\" + str(l)]\n",
    "\n",
    "        #forward propogation \n",
    "        results[\"A\"+ str(l)] = forward_prop(results[\"A\" + str(l-1)], W, b)\n",
    "        results[\"A\"+ str(l)] = activation_function( results[\"A\"+ str(l)], activation=\"relu\")\n",
    "\n",
    "    results[\"A\" + str(no_of_layers)] = forward_prop(results[\"A\" + str(no_of_layers-1 )], param[\"W\"+str(no_of_layers)], param[\"b\"+str(no_of_layers)])\n",
    "    results[\"A\"+ str(no_of_layers)] = activation_function( results[\"A\"+ str(no_of_layers)], activation=\"sigmoid\")\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = len(Y)\n",
    "\n",
    "    for i in results[\"A\"+ str(no_of_layers)]:\n",
    "        if i>0.5:\n",
    "            i=1\n",
    "        else:\n",
    "            i=0\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "        if Y[i] == results[\"A\"+ str(no_of_layers)][i]:\n",
    "            correct_count+=1\n",
    "\n",
    "    accuracy = correct_count/total_count\n",
    "\n",
    "    print(f\"accuracy: {accuracy}\")\n",
    "    \n",
    "    cost = cost_function(results[\"A\"+ str(no_of_layers)], Y)\n",
    "    print(f\"cost:{cost}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jdMUqLBSh6F1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import scipy\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from dnn_app_utils_v3 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(209, 64, 64, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n",
    "\n",
    "train_x_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "train_x.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (4,209) and (12288,209) not aligned: 209 (dim 1) != 12288 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-fec0814f4ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-e98c582d09b0>\u001b[0m in \u001b[0;36mtraining_model\u001b[0;34m(no_of_layers, nL, X, Y, learning_rate, EPOCHS)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m#forward propogation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Z\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A\"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Z\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-933d5ada6032>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(A_prev, W, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,209) and (12288,209) not aligned: 209 (dim 1) != 12288 (dim 0)"
     ]
    }
   ],
   "source": [
    "nL = [train_x.shape[1], 4,i 3, 1]\n",
    "\n",
    "params = training_model(4, nL, train_x, train_y, 0.01, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "numpy_implementation_of_deeepNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
